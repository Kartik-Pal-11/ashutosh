{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjKURzCDSZH6",
        "outputId": "ab74187e-a416-46f8-8ab1-716d9085d40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Value Function\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Final Value function:\n",
            "[[0.54202581 0.49880303 0.47069551 0.4568515 ]\n",
            " [0.55845085 0.         0.35834799 0.        ]\n",
            " [0.59179866 0.64307976 0.6152075  0.        ]\n",
            " [0.         0.7417204  0.86283741 0.        ]]\n",
            "\n",
            "Final Optimal policy:\n",
            "[[0 3 3 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d25f45512d14>:40: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  optimal_policy = np.zeros(num_states, dtype=np.int)\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Define the value function as a vector with size (num_states)\n",
        "num_states = env.observation_space.n\n",
        "V = np.zeros(num_states)\n",
        "print(\"Initial Value Function\")\n",
        "print(V.reshape(4, 4))\n",
        "print()\n",
        "\n",
        "# Define the parameters of the value iteration algorithm\n",
        "gamma = 0.99  # discount factor\n",
        "epsilon = 1e-8  # convergence threshold\n",
        "num_iterations = 100000  # maximum number of iterations\n",
        "\n",
        "# Define a function to update the value of a state based on the Bellman equation\n",
        "def bellman_update(V, state, gamma):\n",
        "    action_values = np.zeros(env.action_space.n)\n",
        "    for action in range(env.action_space.n):\n",
        "        for prob, next_state, reward, done in env.P[state][action]:\n",
        "            action_values[action] += prob * (reward + gamma * V[next_state])\n",
        "    return np.max(action_values)\n",
        "\n",
        "# Run the value iteration algorithm\n",
        "for i in range(num_iterations):\n",
        "    delta = 0\n",
        "    for state in range(num_states):\n",
        "        v = V[state]\n",
        "        V[state] = bellman_update(V, state, gamma)\n",
        "        delta = max(delta, abs(v - V[state]))\n",
        "    if delta < epsilon:\n",
        "        break\n",
        "\n",
        "# Print the learned value function and optimal policy\n",
        "print(\"Final Value function:\")\n",
        "print(V.reshape(4, 4))\n",
        "optimal_policy = np.zeros(num_states, dtype=np.int)\n",
        "for state in range(num_states):\n",
        "    action_values = np.zeros(env.action_space.n)\n",
        "    for action in range(env.action_space.n):\n",
        "        for prob, next_state, reward, done in env.P[state][action]:\n",
        "            action_values[action] += prob * (reward + gamma * V[next_state])\n",
        "    optimal_policy[state] = np.argmax(action_values)\n",
        "print(\"\\nFinal Optimal policy:\")\n",
        "print(optimal_policy.reshape((4, 4)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDQ8VGn_Uk1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}