{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy3MowmrSZoe",
        "outputId": "74541582-cee3-46c6-a999-22f0c4c02dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Q-value function:\n",
            "[[3.33282228e-03 8.81938799e-03 5.42125199e-03 7.79768134e-03]\n",
            " [5.08353843e-03 1.26459114e-02 1.30918624e-02 1.71345062e-02]\n",
            " [3.22612498e-02 2.28074691e-02 2.74372501e-02 1.52626199e-02]\n",
            " [3.66241889e-03 1.39209264e-02 0.00000000e+00 0.00000000e+00]\n",
            " [2.88856700e-03 9.64342486e-03 3.49453659e-03 4.14207209e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [6.02427211e-02 4.73421498e-02 3.68228053e-02 3.17726057e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [5.28174880e-04 1.21500000e-02 2.29466923e-02 1.19656436e-02]\n",
            " [7.57967971e-03 1.13247307e-01 5.33740909e-02 6.21072210e-02]\n",
            " [1.87100526e-01 1.35244904e-01 1.75068311e-01 4.41399416e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.44657483e-02 0.00000000e+00 2.18544046e-01 0.00000000e+00]\n",
            " [2.22850691e-01 5.13161479e-01 5.44214900e-01 3.04992146e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "\n",
            "Final Optimal policy:\n",
            "[[1 3 0 1]\n",
            " [1 0 0 0]\n",
            " [2 1 0 0]\n",
            " [0 2 2 0]]\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Define the parameters of the Monte Carlo Control algorithm\n",
        "num_episodes = 10000\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.2  # exploration probability\n",
        "\n",
        "# Define the Q-value function as a matrix with size (num_states, num_actions)\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Define a dictionary to store the returns for each state-action pair\n",
        "returns = {}\n",
        "\n",
        "# Define a function to select an action based on the Q-value function and exploration-exploitation strategy\n",
        "def select_action(state):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()  # explore\n",
        "    else:\n",
        "        return np.argmax(Q[state])  # exploit\n",
        "\n",
        "# Define the main loop of the Monte Carlo Control algorithm\n",
        "for episode in range(num_episodes):\n",
        "    # Initialize episode variables\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_history = []\n",
        "\n",
        "    # Generate an episode by following the current policy\n",
        "    while not done:\n",
        "        action = select_action(state)  # decaying epsilon-greedy exploration\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_history.append((state, action, reward))\n",
        "        state = next_state\n",
        "\n",
        "    # Update the Q-value function using the episode history\n",
        "    G = 0  # total discounted reward\n",
        "    for t in reversed(range(len(episode_history))):\n",
        "        state, action, reward = episode_history[t]\n",
        "        G = gamma * G + reward\n",
        "        state_action = (state, action)\n",
        "        if state_action not in [(x[0], x[1]) for x in episode_history[0:t]]:\n",
        "            if state_action not in returns:\n",
        "                returns[state_action] = [G]\n",
        "            else:\n",
        "                returns[state_action].append(G)\n",
        "            Q[state][action] = np.mean(returns[state_action])\n",
        "\n",
        "# Print the learned Q-value function and optimal policy\n",
        "print(\"Final Q-value function:\")\n",
        "print(Q)\n",
        "optimal_policy = np.argmax(Q, axis=1)\n",
        "print(\"\\nFinal Optimal policy:\")\n",
        "print(optimal_policy.reshape((4,4)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZMWLHXRUrxH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}