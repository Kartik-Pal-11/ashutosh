{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c729f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 [0.0, 100.0, 0.0, 100.0]\n",
      "Iteration 2 [45.0, 190.0, 9.0, 190.0]\n",
      "Iteration 3 [89.55, 271.0, 24.39, 271.0]\n",
      "Iteration 4 [132.9255, 343.9, 44.1459, 343.9]\n",
      "Iteration 5 [174.620655, 409.51, 66.709179, 409.51]\n",
      "Iteration 6 [214.29863055, 468.559, 90.89033499000001, 468.559]\n",
      "Iteration 7 [251.7522007455, 521.7031, 115.79148134190002, 521.7031]\n",
      "Iteration 8 [286.872561603855, 569.53279, 140.74437888693902, 569.53279]\n",
      "Iteration 9 [319.6247259991226, 612.579511, 165.26089799842063, 612.579511]\n",
      "Iteration 10 [350.0281840492893, 651.3215599, 188.99348336872072, 651.3215599]\n"
     ]
    }
   ],
   "source": [
    "def bellman_equation(state_values, rewards, transitions, gamma):\n",
    "    \"\"\"\n",
    "    Computes the state values using the Bellman equation.\n",
    "\n",
    "    Args:\n",
    "        state_values (list): List of state values, one for each state.\n",
    "        rewards (list): List of rewards, one for each state.\n",
    "        transitions (list): List of transition dictionaries, one for each state.\n",
    "            Each transition dictionary contains the next states and probabilities for each action.\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        A list of updated state values.\n",
    "    \"\"\"\n",
    "    new_state_values = []\n",
    "    for state in range(len(state_values)):\n",
    "        q_values = []\n",
    "        for action in transitions[state]:\n",
    "            next_states = transitions[state][action]\n",
    "            q = 0\n",
    "            for next_state in next_states:\n",
    "                probability = next_states[next_state]\n",
    "                reward = rewards[next_state]\n",
    "                q += probability * (reward + gamma * state_values[next_state])\n",
    "            q_values.append(q)\n",
    "        new_state_values.append(max(q_values))\n",
    "    return new_state_values\n",
    "\n",
    "# Example usage\n",
    "state_values = [0, 0, 0, 0]\n",
    "rewards = [0, 0, 0, 100]\n",
    "transitions = [\n",
    "    {'a': {1: 0.5, 2: 0.5}},\n",
    "    {'b': {3: 1}},\n",
    "    {'c': {1: 0.1, 2: 0.9}},\n",
    "    {'d': {3: 1}}\n",
    "]\n",
    "gamma = 0.9\n",
    "\n",
    "for i in range(10):\n",
    "    state_values = bellman_equation(state_values, rewards, transitions, gamma)\n",
    "    print(\"Iteration\", i+1, state_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f1b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
